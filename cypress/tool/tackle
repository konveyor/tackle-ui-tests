#!/usr/bin/env python3

import argparse
import base64
import copy
from Crypto import Random
from Crypto.Cipher import AES
import datetime
import hashlib
import json
import os
import re
import requests
import shutil
import time
import yaml

###############################################################################

parser = argparse.ArgumentParser(description='Konveyor Tackle maintenance tool.')
parser.add_argument('action', type=str, nargs='*',
                    help='One or more Tackle commands that should be executed, options: export export-tackle1 import clean clean-all')
parser.add_argument('-c','--config', type=str, help='A config file path (tackle-config.yml by default).',
                    nargs='?', default='./tackle-config.yml')
parser.add_argument('-d','--data-dir', type=str, help='Local Tackle data directory path (tackle-data by default).',
                    nargs='?', default='./tackle-data')
parser.add_argument('-v','--verbose', dest='verbose', action='store_const', const=True, default=False,
                    help='Print verbose output (including all API requests).')
parser.add_argument('-s','--skip-destination-check', dest='skipDestCheck', action='store_const', const=True, default=False,
                    help='Skip connection and data check of Tackle 2 destination.')
parser.add_argument('-w','--disable-ssl-warnings', dest='disableSslWarnings', action='store_const', const=True, default=False,
                    help='Do not display warnings during ssl check for api requests.')
parser.add_argument('-i','--ignore-import-errors', dest='ignoreImportErrors', action='store_const', const=True, default=False,
                    help='Skip to next item if an item fails load.')
parser.add_argument('-n','--no-auth', dest='noAuth', action='store_const', const=True, default=False,
                    help='Skip Keycloak token creation, use empty Auth token in Tackle API calls.')
parser.add_argument('-b','--skip-buckets', dest='skipBuckets', action='store_const', const=True, default=False,
                    help='Skip Tackle 2 Buckets content export.')
parser.add_argument('-t','--token', type=str, help='Bearer auth token for Hub API (login/password is a fallback to create it).',
                    nargs='?', default='')
parser.add_argument('-p','--pathfinder-url', type=str, help='In-cluster Pathfinder endpoint URL.',
                    nargs='?', default='')
args = parser.parse_args()

###############################################################################

EXPORT_MANIFEST_FILENAME = "_manifest"
KNOWN_CRYPT_STRING = "tackle-cli-known-string-plaintext"
TOKEN_REFRESH_SECONDS = 240 # 4 minutes to refresh auth token

###############################################################################

def disableSSlWarnings(disableSslWarnings):
    if disableSslWarnings:
        requests.packages.urllib3.disable_warnings()

def ensureDataDir(dataDir):
    # Main dataDir
    if os.path.isdir(dataDir):
        debugPrint("Data directory already exists, using %s" % dataDir)
    else:
      debugPrint("Creating data directory at %s" % dataDir)
      os.mkdir(dataDir)
    # Buckets dump dataDir
    bucketsDir = "%s/buckets" % dataDir
    if os.path.isdir(bucketsDir):
        debugPrint("Buckets directory already exists, cleaning it: %s" % bucketsDir)
        shutil.rmtree(bucketsDir)
    debugPrint("Creating empty bucket directory at %s" % bucketsDir)
    os.mkdir(bucketsDir)

def checkConfig(expected_vars):
    for varKey in expected_vars:
        if os.environ.get(varKey) is None:
            print("ERROR: Missing required environment variable %s, define it first." % varKey)
            exit(1)

def loadConfig(path):
    debugPrint("Loading config from: %s" % path)
    try:
        data = open(path, 'r')
        return yaml.safe_load(data)
    except Exception as ex:
        print("ERROR reading config file %s: %s" % (path, ex))
        exit(1)

def debugPrint(str):
    if args.verbose:
        print(str)

def getHubToken(host, username, password, token):
    if token:
        print("Skipping login, using provided auth token.")
        return token
    else:
        print("Getting auth token via Hub from %s" % host)
        url  = "%s/hub/auth/login" % host
        data = '{"user": "%s", "password": "%s"}' % (username, password)

        r = requests.post(url, data=data, verify=False)
        if r.ok:
            respData = json.loads(r.text)
            debugPrint("Got access token: %s" % respData['token'])
            return respData['token']
        else:
            print("ERROR getting auth token from %s" % url)
            print(data, r)
            exit(1)

def getKeycloakToken(host, username, password, client_id='tackle-ui', realm='tackle'):
    if args.noAuth:
        print("Skipping auth token creation for %s, using empty." % host)
        return ""

    print("Getting auth token from %s" % host)
    url  = "%s/auth/realms/%s/protocol/openid-connect/token" % (host, realm)
    data = {'username': username, 'password': password, 'client_id': client_id, 'grant_type': 'password'}

    r = requests.post(url, data=data, verify=False)
    if r.ok:
        respData = json.loads(r.text)
        debugPrint("Got access token: %s" % respData['access_token'])
        return respData['access_token']
    else:
        print("ERROR getting auth token from %s" % url)
        print(data, r)
        exit(1)

def tackle2path(obj):
    return "/hub/%s" % obj

def loadDump(path, fallback_value = []):
    try:
        data = open(path)
        return json.load(data)
    except Exception as ex:
        print("WARNING: Cannot open dump file %s, assuming empty data." % path)
        debugPrint(ex)
        return fallback_value


def saveJSON(path, jsonData):
    dumpFile = open(path + ".json", 'w')
    dumpFile.write(json.dumps(jsonData, indent=4, default=vars))
    dumpFile.close()

def cmdWanted(args, action):
    if action in args.action:
        return True
    else:
        return False

###############################################################################

class TackleTool:
    # TYPES order matters for import/upload to Tackle2
    TYPES = ['tagcategories', 'tags', 'jobfunctions', 'migrationwaves', 'stakeholdergroups', 'stakeholders', 'businessservices', 'identities', 'applications', 'proxies', 'dependencies', 'questionnaires', 'archetypes', 'assessments', 'reviews']
    NOT_IMPORTED_TYPES = ['taskgroups', 'tasks']
    TACKLE2_SEED_TYPES = ['tagcategories', 'tags', 'jobfunctions']

    def __init__(self, dataDir, tackle2Url, tackle2Token, encKey = ""):
        self.dataDir      = dataDir
        self.Url   = tackle2Url
        # Gather Keycloak access token for Tackle
        self.Token =  getHubToken(tackle2Url, c.get('username', ''), c.get('password', ''), tackle2Token)
        self.TokenRenewAfter = int(time.time()) + TOKEN_REFRESH_SECONDS

        self.encKeyVerified = False
        if encKey != "":
            self.encKey = hashlib.sha256(encKey.encode('utf-8')).digest()

        # Dump data
        self.data         = dict()
        for t in self.TYPES:
            self.data[t] = []
        self.data['origin-tags'] = []   # temp storage for origin tags id remapping
        self.data['temp-buckets'] = []  # temp storage for bucket owner's refs during export
        # Existing resources in destination
        self.destData        = dict()
        for t in self.TYPES:
            self.destData[t] = dict()

    def findById(self, objType, id):
        # Search in data to be imported
        for obj in self.data[objType]:
            if obj.id == id:
                return obj
        # Raise error if still not found
        print("ERROR: %s record ID %d not found." % (objType, id))
        exit(1)

    def checkTokenLifetime(self):
        if self.TokenRenewAfter < int(time.time()):
             self.Token =  getHubToken(self.Url, c.get('username', ''), c.get('password', ''), False)
             self.TokenRenewAfter = int(time.time()) + TOKEN_REFRESH_SECONDS

    def apiJSON(self, url, data=None, method='GET', ignoreErrors=False):
        debugPrint("Querying: %s" % url)
        self.checkTokenLifetime()
        if method == 'DELETE':
            r = requests.delete(url, headers={"Authorization": "Bearer %s" % self.Token, "Content-Type": "application/json"}, verify=False)
        elif method == 'POST':
            debugPrint("POST data: %s" % json.dumps(data))
            r = requests.post(url, data=json.dumps(data), headers={"Authorization": "Bearer %s" % self.Token, "Content-Type": "application/json"}, verify=False)
        elif method == 'PATCH':
            debugPrint("PATCH data: %s" % json.dumps(data))
            r = requests.patch(url, data=json.dumps(data), headers={"Authorization": "Bearer %s" % self.Token, "Content-Type": "application/json"}, verify=False)
        elif method == 'PUT':
            debugPrint("PUT data: %s" % json.dumps(data))
            r = requests.put(url, data=json.dumps(data), headers={"Authorization": "Bearer %s" % self.Token, "Content-Type": "application/json"}, verify=False)
        else: # GET
            r = requests.get(url, headers={"Authorization": "Bearer %s" % self.Token, "Content-Type": "application/json"}, verify=False)
    
        if not r.ok:
            if ignoreErrors:
                debugPrint("Got status %d for %s, ignoring" % (r.status_code, url))
            else:
                print("ERROR: API request failed with status %d for %s" % (r.status_code, url))
                exit(1)
    
        if r.text is None or r.text ==  '':
            return
    
        debugPrint("Response: %s" % r.text)
    
        respData = json.loads(r.text)
        if '_embedded' in respData:
            debugPrint("Unwrapping Tackle1 JSON")
            return respData['_embedded'][url.rsplit('/')[-1].rsplit('?')[0]] # unwrap Tackle1 JSON response (e.g. _embedded -> application -> [{...}])
        else:
            return respData # raw return JSON (Tackle2, Pathfinder)
    
    def apiRaw(self, url, data=None, method='GET', ignoreErrors=False):
        debugPrint("Querying: %s" % url)
        self.checkTokenLifetime()
        if method == 'DELETE':
            r = requests.delete(url, headers={"Authorization": "Bearer %s" % self.Token}, verify=False)
        elif method == 'POST':
            debugPrint("POST data: %s" % json.dumps(data))
            r = requests.post(url, data=json.dumps(data), headers={"Authorization": "Bearer %s" % self.Token}, verify=False)
        elif method == 'PATCH':
            debugPrint("PATCH data: %s" % json.dumps(data))
            r = requests.patch(url, data=json.dumps(data), headers={"Authorization": "Bearer %s" % self.Token}, verify=False)
        else: # GET
            r = requests.get(url, headers={"Authorization": "Bearer %s" % self.Token}, verify=False)
    
        if not r.ok:
            if ignoreErrors:
                debugPrint("Got status %d for %s, ignoring" % (r.status_code, url))
            else:
                print("ERROR: API request failed with status %d for %s" % (r.status_code, url))
                exit(1)
    
        return r.text
    
    def apiFileGet(self, url, destinationPath, ignoreErrors=False):
        debugPrint("Getting file from %s" % url)
        self.checkTokenLifetime()
        # Get via streamed request
        with requests.get(url, headers={"Authorization": "Bearer %s" % self.Token, "X-Directory": "archive"}, verify=False, stream=True) as resp:
            # Check for errors
            if not resp.ok:
                if ignoreErrors:
                    debugPrint("Got status %d for get file %s, ignoring" % (resp.status_code, url))
                else:
                    print("ERROR: File get API request failed with status %d for %s" % (resp.status_code, url))
                    exit(1)
            # Store to local destination file
            with open(destinationPath, 'wb') as destFile:
                shutil.copyfileobj(resp.raw, destFile)
            destFile.close
    
        return destFile.name
    
    def apiFilePost(self, url, filePath, ignoreErrors=False):
        debugPrint("Uploading file %s to %s" % (filePath, url))
        self.checkTokenLifetime()
        # Open local file
        with open(filePath, 'rb') as f:
            # Upload the content
            resp = requests.post(url, files={'file': f}, headers={"Authorization": "Bearer %s" % self.Token, "X-Directory": "expand"}, verify=False)
            # Check for errors
            if not resp.ok:
                if ignoreErrors:
                    debugPrint("Got status %d for post file %s, ignoring" % (resp.status_code, url))
                else:
                    print("ERROR: File post API request failed with status %d for %s" % (resp.status_code, url))
                    exit(1)
        return resp.text

    # Gather Tackle 2 API objects
    def dumpTackle2(self):
        ensureDataDir(self.dataDir)
        for t in self.TYPES:
            print("Exporting %s.." % t)
            if t == "identities":
                dictCollection = self.apiJSON(self.Url + "/hub/identities?decrypted=1")
                for dictObj in dictCollection:
                    dictObj['key'] = self.encrypt(dictObj['key'])
                    dictObj['password'] = self.encrypt(dictObj['password'])
                    dictObj['settings'] = self.encrypt(dictObj['settings'])
            else:
                dictCollection = self.apiJSON(self.Url + tackle2path(t))

            # Remove legacy locked questionnaire from export to not cause conflict in import (should be 1st one)
            if t == "questionnaires":
                dictCollection = dictCollection[1:]

            # Save data locally
            saveJSON(os.path.join(self.dataDir, t), dictCollection)

    def dumpTackle2Buckets(self):
        bucketDir = "%s/buckets" % self.dataDir
        if not os.path.exists(bucketDir):
            os.mkdir(bucketDir)
        for bucket in self.data['temp-buckets']:
            debugPrint("Downloading bucket content for %s" % bucket['owner'])
            bucketFilename = bucket['owner'].replace("/", "--")
            self.apiFileGet(self.Url + "/hub/" + bucket['owner'] + "/bucket/", bucketDir + "/%s.tar.gz" % bucketFilename)

    def uploadTackle2Buckets(self):
        bucketDir = "%s/buckets/" % self.dataDir
        if not os.path.exists(bucketDir):
            print("Warning: Buckets directory %s doesn't exist, skipping." % bucketDir)
            return
        for bucketArchive in os.listdir(bucketDir):
            ownerPath = bucketArchive.replace("--", "/").replace(".tar.gz", "")
            if os.path.getsize(bucketDir + bucketArchive) > 0:
                print("Uploading bucket archive for %s.." % ownerPath)
                self.apiFilePost(self.Url + "/hub/" + ownerPath + "/bucket/", bucketDir + bucketArchive)
            else:
                debugPrint("Skipping empty bucket archive upload %s" % bucketArchive)

    def add(self, type, item):
        for existingItem in self.data[type]:
            if hasattr(item, 'id') and item.id == existingItem.id:  # assessment links objects don't have primary key id
                # The item is already present, skipping
                return
        self.data[type].append(item)

    def uploadTackle2(self, ignoreErrors=False):
        for t in self.TYPES:
            dictCollection = loadDump(os.path.join(self.dataDir, t + '.json'))
            print("Uploading %s.." % t)
            for dictObj in dictCollection:
                # Decrypt Identity fields
                if "identities" in t:
                    dictObj['key'] = self.decrypt(dictObj['key'])
                    dictObj['password'] = self.decrypt(dictObj['password'])
                    dictObj['settings'] = self.decrypt(dictObj['settings'])

                if "stakeholdergroups" in t:
                    dictObj['stakeholders'] = [] # empty stakeholders to not create it with parent stakeholdergroup, but in separate call

                if "stakeholders" in t:
                     # Empty stakeholders Refs to Application, linked from Application created later
                    dictObj['owns'] = []
                    dictObj['contributes'] = []

                if "migrationwaves" in t:
                    # Empty migrationvawe's Refs to avoid circular dependency problem, association is linked from the opposite side
                    dictObj['applications'] = []
                    dictObj['stakeholders'] = []
                    dictObj['stakeholdergroups'] = []

                path = tackle2path(t)
                if "assessments" in t:
                    if 'application' in dictObj:
                        path = tackle2path("applications/%d/assessments" % dictObj['application']['id'])
                    elif 'archetype' in dictObj:
                        path = tackle2path("archetypes/%d/assessments" % dictObj['archetype']['id'])
                debugPrint(dictObj)
                self.apiJSON(self.Url + path, dictObj, method='POST', ignoreErrors=ignoreErrors)


    # Migrate Pathfinder Assessment to Konveyor (expecting Pathfinder hard-coded questionnaire ID=1)
    def migrateAssessments(self, pathfinderUrl, ignoreErrors=False):
        cnt = 0
        apps = self.apiJSON(self.Url + "/hub/applications")
        print("There are %d Applications, looking for their Assessments.." % len(apps))
        for app in apps:
            # Export Pathfinder data for each Application
            for passmnt in self.apiJSON(pathfinderUrl + "/assessments?applicationId=%d" % app['id']):
                print("# Assessment for Application %s" % passmnt["applicationId"])
                appAssessmentsPath = "/hub/applications/%d/assessments" % passmnt["applicationId"]
                # Skip if Assessment for given Application already exists
                if len(self.apiJSON(self.Url + appAssessmentsPath, data={"questionnaire": {"id": 1}})) > 0:
                    print("  Assessment already exists, skipping.")
                    continue

                # Prepare new Assessment
                assmnt = dict()
                assmnt['questionnaire'] = {"id": 1} # Default new Questionnaire "Pathfinder Legacy"
                assmnt['application'] = {"id": passmnt["applicationId"]}
                assmnt['stakeholders'] = []
                for sh in passmnt['stakeholders']:
                    assmnt['stakeholders'].append({"id": sh})
                assmnt['stakeholderGroups'] = []
                for shg in passmnt['stakeholderGroups']:
                    assmnt['stakeholderGroups'].append({"id": shg})

                # Transformate Questions, Answers and related structures
                for category in passmnt['questionnaire']['categories']:
                    del category['id']
                    category['name'] = category.pop('title')
                    for question in category["questions"]:
                        del question['id']
                        question["text"] = question.pop('question')
                        question["explanation"] = question.pop('description')
                        question["answers"] = question.pop('options')
                        for answer in question['answers']:
                            del answer['id']
                            answer['text'] = answer.pop('option')
                            answer['selected'] = answer.pop('checked')
                            answer['risk'] = answer['risk'].lower()
                            if answer['risk'] == "amber":
                                answer['risk'] = "yellow"
                assmnt['sections'] = passmnt['questionnaire']['categories']

                # Post the Assessment
                self.apiJSON(self.Url + appAssessmentsPath, data=assmnt, method='POST')
                cnt += 1
                print("Assessment submitted.")
        return cnt

    def preImportCheck(self):
        # Compatibility checks
        # TagCategories on Hub API
        if self.apiJSON(self.Url + "/hub/tagcategories", ignoreErrors=True) is None:
            print("ERROR: The API doesn't know TagCategories, use older version of this tool.")
            exit(1)

        # TagCategories in the data directory
        if not os.path.exists(os.path.join(self.dataDir, 'tagcategories.json')) and os.path.exists(os.path.join(self.dataDir, 'tagtypes.json')):
            print("ERROR: The dump comes from old version using TagTypes instead of TagCategories. Update the dump first.")
            print("Recommended steps to update the dump: (make a backup copy of the data directory first)")
            print("  cd %s" % self.dataDir)
            print("  mv tagtypes.json tagcategories.json")
            print("  sed -i 's/tagType/category/g' tags.json")
            print("  cd ..")
            print("and run import again.")
            exit(1)

        # Duplication checks
        for t in self.TYPES:
            print("Checking %s in destination Tackle2.." % t)
            destCollection = self.apiJSON(self.Url + tackle2path(t))
            localCollection = loadDump(os.path.join(self.dataDir, t + '.json'))
            for importObj in localCollection:
                for destObj in destCollection:
                    if importObj['id'] == destObj['id']:
                        print("ERROR: Resource %s/%d \"%s\" already exists in Tackle2 destination as \"%s\". Clean it before running the import with: tackle clean" % (t, importObj['id'], importObj['name'], destObj['name']))
                        exit(1)

    def cleanTackle2(self):
        self.TYPES.reverse()
        for t in self.TYPES:
            dictCollection = loadDump(os.path.join(self.dataDir, t + '.json'))
            for dictObj in dictCollection:
                # Hub resources
                print("Trying delete %s/%s" % (t, dictObj['id']))
                self.apiJSON("%s/hub/%s/%d" % (self.Url, t, dictObj['id']), method='DELETE', ignoreErrors=True)

    def cleanAllTackle2(self):
        self.TYPES.reverse()
        for t in self.NOT_IMPORTED_TYPES + self.TYPES:
            destCollection = self.apiJSON(self.Url + tackle2path(t))
            for dictObj in destCollection:
                # Hub resources
                print("Deleting %s/%s" % (t, dictObj['id']))
                self.apiJSON("%s/hub/%s/%d" % (self.Url, t, dictObj['id']), method='DELETE', ignoreErrors=True)

    def encrypt(self, plain):
        if plain == "":
            return ""
        iv = Random.new().read(AES.block_size)
        cipher = AES.new(self.encKey, AES.MODE_CFB, iv)
        return base64.b64encode(iv + cipher.encrypt(bytes(plain, 'utf-8'))).decode('ascii')

    def decrypt(self, encrypted, verifyEncKey = True):
        # Check key from config file to be matching to key from export manifest to ensure decryption consistency
        if verifyEncKey and not self.encKeyVerified:
            if self.verifyManifestEncryptKey():
                self.encKeyVerified = True
            else:
                print("Decryption failure: inconsistent encryption_passphase in config, get more details with -v / --verbose option.")
                exit(1)
        # Decrypt the input
        encrypted_bytes = base64.b64decode(encrypted)
        if encrypted_bytes == b'':
            return ""
        iv = encrypted_bytes[:AES.block_size]
        cipher = AES.new(self.encKey, AES.MODE_CFB, iv)
        return cipher.decrypt(encrypted_bytes[AES.block_size:]).decode('utf-8')

    def saveManifest(self):
        mf = dict()
        mf['timestamp_utc'] = datetime.datetime.utcnow().isoformat()
        mf['command_options'] = args
        mf['known_string'] = KNOWN_CRYPT_STRING
        mf['crypted_known_string'] = self.encrypt(KNOWN_CRYPT_STRING)
        debugPrint("Saving manifest as %s" % EXPORT_MANIFEST_FILENAME)
        saveJSON(os.path.join(self.dataDir, EXPORT_MANIFEST_FILENAME), mf)

    def verifyManifestEncryptKey(self):
        debugPrint("Verifying manifest from %s" % EXPORT_MANIFEST_FILENAME)
        mf = loadDump(os.path.join(self.dataDir, EXPORT_MANIFEST_FILENAME + ".json"), {})
        if 'known_string' in mf and 'crypted_known_string' in mf and mf['crypted_known_string'] != "":
            if self.decrypt(mf['crypted_known_string'], False) == mf['known_string']:
                debugPrint("Encryption key check succeessful.")
                return True
            else:
                debugPrint("Encryption key check failure - incorrect passphase.")
                return False
        else:
            debugPrint("Encryption key check failure - empty or missing passphase.")
            return False


class Tackle2Object:
    def __init__(self, initAttrs = {}):
        if initAttrs:
            self.id         = initAttrs['id']
            self.createUser = initAttrs['createUser']
            self.updateUser = initAttrs['updateUser']

###############################################################################
# Disable SSL warnings if needed
disableSSlWarnings(args.disableSslWarnings)

# Load YAML config file (tackle-config.yml)
c = loadConfig(args.config)
cmdExecuted = False

# Tackle 2 export steps
if cmdWanted(args, "export"):
    cmdExecuted = True

    # Setup data migration object
    tool = TackleTool(args.data_dir, c['url'], args.token, c['encryption_passphase'])

    # Run the export expecting clean destination
    print("Exporting Tackle 2 objects into %s (this might take a while..)" % args.data_dir)
    tool.dumpTackle2()
    tool.saveManifest()
    if args.skipBuckets:
        print("Skipping Buckets file content export.")
    else:
        print("Downloading Bucket content into %s/buckets" % args.data_dir)
        tool.dumpTackle2Buckets()
    print("Done. The data could be imported to another Tackle 2 using command \"tackle clean-all && tackle import\"")


# Tackle 2 import steps
if cmdWanted(args, "import"):
    cmdExecuted = True

    # Setup Tackle 1.2->2.0 data migration object
    tool = TackleTool(args.data_dir, c['url'], args.token, c['encryption_passphase'])

    # Run the import
    print("Importing data to Tackle2")
    if not args.skipDestCheck:
        tool.preImportCheck()
    tool.uploadTackle2(ignoreErrors=args.ignoreImportErrors)

    if args.skipBuckets:
        print("Skipping Buckets content content import.")
    else:
        print("Uploading Bucket content into %s/buckets" % args.data_dir)
        tool.uploadTackle2Buckets()
    print("Done. Open your Tackle2 now!")

# Clean created objects in Tackle2
if cmdWanted(args, "clean"):
    cmdExecuted = True
    
    # Setup Tackle 1.2->2.0 data migration object
    tool = TackleTool(args.data_dir, c['url'], args.token)

    # Run the cleanup
    print("Cleaning data created in Tackle2")
    tool.cleanTackle2()
    print("Done. Records from local JSON data files were removed from Tackle2 API.")

# Clean ALL objects in Tackle2
if cmdWanted(args, "clean-all"):
    cmdExecuted = True
    
    # Setup Tackle 1.2->2.0 data migration object
    tool = TackleTool(args.data_dir, c['url'], args.token)

    # Run the cleanup including seeds
    print("Cleaning ALL data in Tackle2")
    tool.cleanAllTackle2()

# Migrate Pathfinder Assessments to Konveyor Assessments
if cmdWanted(args, "migrate-assessments"):
    cmdExecuted = True

    # Check Pathfinder URL arg
    if not args.pathfinder_url:
        print("Error: Pathfinder URL is required, specify it with -p or --pathfinder-url option.")
        exit(1)

    # Setup Tackle data migration object
    tool = TackleTool(args.data_dir, c['url'], args.token)

    # Run the import
    print("Starting Pathfinder Assessments to Konveyor Assessment migration.")
    appCnt = tool.migrateAssessments(args.pathfinder_url)

    print("Done. %d new Assessment(s) for Application(s) were migrated!" % appCnt)


# Print help if action was not specified
if not cmdExecuted:
    print("Unknown action, use tackle --help to see usage.")
    exit(1)

###############################################################################